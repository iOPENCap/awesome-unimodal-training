# Awesome-UniModal-Training
The project is used to store text-only training, language-free training for multimodal tasks related papers.

Include:
- text-only training for zero-shot **image/audio/video captioning, composed image retrieval, visual storytelling**...
- language-free training for **text-to-image generation**...
## papers
### text-only training
#### <br/>> **2024**

* [**AAAI**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 1] Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training    [[paper]](https://arxiv.org/abs/2401.02347)  [[code]](https://github.com/Artanic30/MacCap)[⭐8]<br/>

* [**ACM**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 1] TOMGPT: Reliable Text-Only Training Approach for Cost-Effective Multi-modal Large Language Model[[paper]](https://dl.acm.org/doi/abs/10.1145/3654674)<br/>

* [**IJCV**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 5]  Learning to Prompt with Text Only Supervision for Vision-Language Models[[paper]](https://arxiv.org/abs/2401.02418)  [[code]](https://github.com/muzairkhattak/ProText)[⭐80]<br/>

* [**arxiv**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 0] Text Data-Centric Image Captioning with Interactive Prompts[[paper]](https://arxiv.org/abs/2403.19193)<br/>
* [**arxiv**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 2]  MeaCap: Memory-Augmented Zero-shot Image Captioning[[paper]](https://www.semanticscholar.org/paper/70faf1731707ddb329877031a00d4b262902ba3c)  [[code]](https://github.com/joeyz0z/MeaCap)[⭐27]<br/>
* [**arxiv**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 0]  ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks    [[paper]](https://arxiv.org/abs/2402.17298)<br/>
* [**arxiv**] | [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 0]  Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion    [[paper]](https://arxiv.org/abs/2407.11211)<br/>

#### <br/>> **2023**
* [**arxiv**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 1] Improved Factorized Neural Transducer Model For text-only Domain Adaptation[[paper]](https://arxiv.org/abs/2309.09524) <br/>
* [**AAAI**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 2] Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning[[paper]](https://arxiv.org/abs/2312.08865)  <br/>
* [**ACM**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 0] Text-Only Training for Visual Storytelling[[paper]](https://arxiv.org/abs/2308.08881)  <br/>
* [**ACM**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 1] VLIS: Unimodal Language Models Guide Multimodal Language Generation[[paper]](https://arxiv.org/abs/2310.09767)  [[code]](https://github.com/jiwanchung/vlis)[⭐24] <br/>

* [**NeurlPS**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 7] LOVM:Language-Only Vision Model Selection[[paper]](https://arxiv.org/abs/2306.08893)  [[code]](https://github.com/orrzohar/LOVM)[⭐18]<br/>
* [**IJCAI**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 4] From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping[[paper]](https://arxiv.org/abs/2304.13273)  [[code]](https://github.com/junyangwang0410/Knight)[⭐11]<br/>
* [**ICLR**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 47] Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training[[paper]](https://arxiv.org/abs/2303.03032)  [[code]](https://github.com/dhg-wei/DeCap)[⭐118]<br/>
* [**DCASE**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 4] Weakly-supervised Automated Audio Captioning via text only training[[paper]](https://arxiv.org/abs/2309.12242)  [[code]](https://github.com/zelaki/wsac)[⭐1]<br/>
* [**ACM**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 3] CgT-GAN: CLIP-guided Text GAN for Image Captioning[[paper]](https://arxiv.org/abs/2308.12045)  [[code]](https://github.com/Lihr747/CgtGAN)[⭐16]<br/>


#### <br/>> **2022**
* [**EMNLP**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 64] Text-Only Training for Image Captioning using Noise-Injected CLIP[[paper]](https://arxiv.org/abs/2211.00575)  [[code]](https://github.com/DavidHuji/CapDec)[⭐179]<br/>

* [**ICCV**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 11] I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision[[paper]](https://arxiv.org/abs/2211.09778)[[code]](https://github.com/allenai/close)[⭐55]<br/>

* [**arxiv**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 28] Multimodal Knowledge Alignment with Reinforcement Learning[[paper]](https://arxiv.org/abs/2205.12630)[[code]](https://github.com/jiwanchung/esper)[⭐22]<br/>
#### <br/>> **2021**

* [**CVPR**] [<img src="https://github.com/user-attachments/assets/c30947ec-4d5a-424d-89eb-583d8efd2801" width="15"> 124] LAFITE: Towards Language-Free Training for Text-to-Image Generation[[paper]](https://arxiv.org/abs/2111.13792)  [[code]](https://github.com/drboog/Lafite)[⭐180]<br/>

### language-free trainning
